{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJVlJllsoDlU"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "This notebook contains an example of a workflow for performing exploratory data analysis to investigate a dataset that we want to use to train an object detection model. The methods then use transfer learning to tune the ```sd_resnet50_v1_fpn_640x640_coco17_tpu-8``` model so that it can detect the new class of object defined in our dataset. I describe the model selected for tuning in great detail later in this notebook.\n",
        "\n",
        "In this case, our training data set contains histology images with annotations that label cells of interest. The original images are not publicly available so they had to be omitted from this notebook.\n",
        "\n",
        "For our training purposes, we heavily revise the methods described in the following tutorial [[Tutorial](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb)] for the purposes of detecting multiple objects instead of the single object detection described in the instructions.\n",
        "\n",
        "As you will see in the section \"Summary of Data Quality\" our training data is significantly flawed, which inhibits out ability to achieve a high level of model accuracy. A important part of model training is to ensure that high quality data is provided, and this notebook is a good example of the \"garbage in, garbage out\" paradigm popular in Data Science. In creating an enterprise solution, this notebook would prove that higher quality data is necessary and I would work with a scientist to obtain a better dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8WkGepVJVH3"
      },
      "source": [
        "### Summary of Data Quality\n",
        "- The raw data is unsuprisingly messy:\n",
        "  - Annotation Data Review:\n",
        "    - Bounding Boxes:\n",
        "      - Bounding boxes are inconsistent and not precise in how they define the objects (ROIs).\n",
        "      - Some boxes are missing, which may be making it difficult for the model to train to identify cells.\n",
        "    - Inconsistent Data Labelling:\n",
        "      - Some annotations are labelled as 'cell', however other annotations are missing this label.\n",
        "        - Need clear understanding of what is labelled so we can properly train our model if multiple classes exist.\n",
        "  - Image Data Review:\n",
        "    - Image Capture:\n",
        "      - Images are under-exposed, which reduces the range of image intensity values available for resolving different structures during training.\n",
        "    - Cell Consistency:\n",
        "      - There is high variability in both the illumination and sharpness of cell imaging. We see very blurry cells and cells that are in focus.\n",
        "      - Some cells observed with internal structures, while others were uniform in appearance.\n",
        "    - Aberrant Data:\n",
        "      - Random, blurred circles exist within some images (Example: See 37.png)\n",
        "        - I'm not sure what this is, but it looks like some sort of post-processing, or possibly optical aberration from the sample preparation.\n",
        "      - Black, straight or curly lines exist in some images.\n",
        "        - Straight lines may be splitting of tissue samples.\n",
        "        - Curly lines have less obvious explaination. It would be good to understand what these are to know if this has any affect on whether the data needs to be discarded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJw6K0ztl6F"
      },
      "source": [
        "## 1. Image Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_NbiXwY6FrD",
        "outputId": "60c6d03b-63e0-461d-da30-36400dbc941f"
      },
      "outputs": [],
      "source": [
        "! pip install matplotlib opencv-python\n",
        "\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfy95sk3nP5g"
      },
      "source": [
        "### 1.1 Loading Images\n",
        "Images are loaded into the notebook for preprocessing and analysis.\n",
        "\n",
        "\n",
        "### Evaluate Data Provided by Scientist and Establish Areas of Improvement\n",
        "\n",
        "#### Here through manual observation of our images, we can tell that some of the data provided has been artificially simulated, where in some cases an image was rotated to increase the size of available training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F41ta1nzwarU",
        "outputId": "f41774f8-a069-40f0-dc93-5c117d79b0d0"
      },
      "outputs": [],
      "source": [
        "image_folder = \"/content/drive/MyDrive/Colab Notebooks/raw_imgs\"\n",
        "annotation_folder = \"/content/drive/MyDrive/Colab Notebooks/raw_anno\"\n",
        "\n",
        "paired_data = []\n",
        "unpaired_data = []\n",
        "\n",
        "# We assume the image file has a corresponding json\n",
        "for img_file in os.listdir(image_folder):\n",
        "    # Here we get the name without an extension\n",
        "    base_name = os.path.splitext(img_file)[0]\n",
        "\n",
        "    img_path = os.path.join(image_folder, img_file)\n",
        "    # We pair it with the .json extension to get the path for the annotation file\n",
        "    anno_path = os.path.join(annotation_folder, base_name + '.json')\n",
        "\n",
        "    # Check if annotation exists for the image\n",
        "    if os.path.exists(anno_path):\n",
        "        image = cv2.imread(img_path)\n",
        "        with open(anno_path, 'r') as f:\n",
        "            annotation = json.load(f)\n",
        "        # make a tuple to pair the data\n",
        "        paired_data.append((image, annotation))\n",
        "\n",
        "    else:\n",
        "      # collect a list of annotations that were not paired with images\n",
        "      unpaired_data.appen(anno_path)\n",
        "\n",
        "if not unpaired_data:\n",
        "  print(\"All annotations present and loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq8PF4qPg5Si"
      },
      "source": [
        "Example Output:\n",
        "\n",
        "\n",
        "```\n",
        "All annotations present and loaded!\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEhzBkUftxhr"
      },
      "source": [
        "\n",
        "### 1.2 Channel Comparison and Redundancy Removal\n",
        "Comparison of image channels is performed to identify and remove redundant data. An important thing to note is that the package we used to import the data actually took single channel data and duplicated it across three channels by default, so this step is redundant. However, if you were working with a unfamiliar package to load your data or had images with multichannel data, this step would be necessary to verify you are not working with redundant data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO-N2eZU3uP7",
        "outputId": "874cf315-b60b-476d-b739-02805a0558be"
      },
      "outputs": [],
      "source": [
        "#check to see whether multiple color channels present\n",
        "paired_data[0][0].shape\n",
        "\n",
        "# Here we suspect its an RGB image because it has three channels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhhf1YPohA2l"
      },
      "source": [
        "Example Output:\n",
        "\n",
        "\n",
        "```\n",
        "(512, 512, 3)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "AWuvwAPM6wWa",
        "outputId": "2f9eae59-d109-4ef0-dc29-a9143a84fc64"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def inspect_channels(image):\n",
        "    if image.ndim == 3 and image.shape[-1] == 3:  # Check if the image has 3 channels\n",
        "        red, green, blue = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
        "        # Create a composite image\n",
        "        composite_image = np.stack((red, green, blue), axis=-1)\n",
        "    else:\n",
        "        print(\"Image does not have 3 channels!\")\n",
        "        return\n",
        "\n",
        "    # Display each channel side by side and the composite image\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(red, cmap='gray')\n",
        "    plt.title('Red Channel')\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.imshow(green, cmap='gray')\n",
        "    plt.title('Green Channel')\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.imshow(blue, cmap='gray')\n",
        "    plt.title('Blue Channel')\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.imshow(composite_image)\n",
        "    plt.title('Composite Image')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compare pixel values between channels\n",
        "    rg_diff = np.abs(red - green)\n",
        "    rb_diff = np.abs(red - blue)\n",
        "    gb_diff = np.abs(green - blue)\n",
        "\n",
        "    print(\"R-G Channel Mean Difference:\", np.mean(rg_diff))\n",
        "    print(\"R-B Channel Mean Difference:\", np.mean(rb_diff))\n",
        "    print(\"G-B Channel Mean Difference:\", np.mean(gb_diff))\n",
        "\n",
        "    if np.sum([np.mean(rg_diff), np.mean(rb_diff), np.mean(gb_diff)]) == 0:\n",
        "        print(\"All channels are the same\")\n",
        "\n",
        "# check to see if there are differences across channels\n",
        "inspect_channels(paired_data[5][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzGGpIlmhE7k"
      },
      "source": [
        "Example Output:\n",
        "> Displays image of the red, green, and blue channel and then a composite image of the overlay of all three channels\n",
        "\n",
        "```\n",
        "R-G Channel Mean Difference: 0.0\n",
        "R-B Channel Mean Difference: 0.0\n",
        "G-B Channel Mean Difference: 0.0\n",
        "All channels are the same\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeRVkNFx79DK",
        "outputId": "ddd9398e-f466-49b8-c48c-ceba5c0d7f4d"
      },
      "outputs": [],
      "source": [
        "# Because all channels are the same, we discard the extra channels to clean up the data a little bit\n",
        "for i in range(len(paired_data)):\n",
        "    paired_data[i] = (paired_data[i][0][:,:,0:1], paired_data[i][1])\n",
        "\n",
        "# we confirm only one channel is left\n",
        "paired_data[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VqCLi1MhVZp"
      },
      "source": [
        "Example Output:\n",
        "\n",
        "\n",
        "```\n",
        "(512, 512, 1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhaRtB5Gt_pQ"
      },
      "source": [
        "### 1.3 Image Data Format Confirmation\n",
        "The format of the images is confirmed to be `float32` with intensity ranges from 0 to 256.\n",
        "\n",
        "\n",
        "### 1.4 Intensity Histogram and Exposure Analysis\n",
        "A histogram is created for all images to analyze the exposure levels, highlighting any underexposure issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswz_T81pfgX"
      },
      "source": [
        "Here we generate some stats to consider the quality of the images\n",
        "The histogram of the batches shows that images were generally underexposed and the scientist could increase exposure conditions to improve the range of available pixel intensity values for training.\n",
        "Manual review of the images shows some black splotches that should be manually removed from a future dataset. We also see some blurry spots that may also be poor quality data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "B0wZr-sF-v-a",
        "outputId": "f17f4a1a-b7b3-47a8-dc78-4a74c80a639d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Image Preprocessing\n",
        "def validate_image(image, target_dtype=np.float32, target_range=(0, 255)):\n",
        "    \"\"\"Ensure the image is in the correct format and range.\"\"\"\n",
        "    image = np.asarray(image, dtype=target_dtype)  # Convert to the target data type\n",
        "    min_val, max_val = np.min(image), np.max(image)  # Check min and max values\n",
        "    if min_val < target_range[0] or max_val > target_range[1]:\n",
        "      print(f\"Image out of range for expected values: {min_val} < {target_range[0]}, {max_val} > {target_range[1]}\")\n",
        "    return image\n",
        "\n",
        "# Single Image Histogram\n",
        "def histogram_for_single_image(image, bins=256, range=(0, 255)):\n",
        "    \"\"\"Prepare histogram for a single image.\"\"\"\n",
        "    flattened_image = image.flatten()  # Flatten the image\n",
        "    hist_values, bin_edges = np.histogram(flattened_image, bins=bins, range=range)\n",
        "    return hist_values, bin_edges\n",
        "\n",
        "# Batch Histogram\n",
        "def histogram_for_batch(images, bins=256, range=(0, 255)):\n",
        "    \"\"\"Prepare histogram for a batch of images.\"\"\"\n",
        "    total_hist_values = np.zeros(bins, dtype=np.float32)  # Initialize histogram values\n",
        "\n",
        "    for image in images:\n",
        "        hist_values, _ = histogram_for_single_image(image, bins=bins, range=range)\n",
        "        total_hist_values += hist_values  # Accumulate histogram values\n",
        "\n",
        "    return total_hist_values\n",
        "\n",
        "def plot_histogram(hist_values, bin_edges, title='Histogram', xlabel='Pixel Intensity', ylabel='Frequency'):\n",
        "    \"\"\"Plot the histogram.\"\"\"\n",
        "    plt.bar(bin_edges[:-1], hist_values, width=np.diff(bin_edges)[0], align='edge')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "# Preprocess all images\n",
        "validated_images = [validate_image(img) for img, _ in paired_data] # All images\n",
        "\n",
        "# Generate histograms\n",
        "for image in validated_images:\n",
        "    hist_values, bin_edges = histogram_for_single_image(image)  # Histogram for individual image\n",
        "\n",
        "# Generate batch histogram\n",
        "hist_values_batch = histogram_for_batch(validated_images)\n",
        "plot_histogram(hist_values_batch, bin_edges, title='Batch Histogram')  # Uncomment to visualize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXQwG3jfhaUd"
      },
      "source": [
        "Example Output:\n",
        "> Displays a histogram of pixel intensity, with a peak on the left side of the graph close to zero, and tail on the right side of the data. This indicates most images are under-exposed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vasA2T_quOrw"
      },
      "source": [
        "### 1.5 Cumulative Distribution Function (CDF) Calculation\n",
        "The CDF is calculated using histogram data to aid in adjusting pixel values for better exposure. Here we use the cumulative distribution function to equalize pixel intensity values. In our case we equalize according to the CDF calculated for an entire batch rather than using the CDF from each image. This is to ensure greater consistency but **requires** batch processing of images\n",
        "\n",
        "#### Why we do this:\n",
        "In statistical terms, the CDF is a function that indicates the probability of a variable taking a value less than or equal to a certain level. When applied to image processing, the CDF maps the intensity levels of pixels in an image to a cumulative probability. This function shows for any given intensity level in the image, the probability of finding a pixel with intensity less than or equal to that level. The histogram of an image shows the frequency of each intensity level. For histogram equalization, we first normalize this histogram to a probability distribution. Then, the CDF is calculated by taking the cumulative sum of these probabilities. The resulting CDF is used to remap the intensity levels of the image's pixels, which redistributes the pixel intensities across the available range more evenly.\n",
        "\n",
        "Applying the CDF in histogram equalization achieves the following:\n",
        "- **Enhances Contrast**: It spreads out the most frequent intensity values, which enhances the global contrast of the image.\n",
        "- **Equalizes Intensities**: The CDF ensures that each intensity level is used in the image, maximizing the use of available intensity levels.\n",
        "- **Improves Visibility**: Details hidden in dark or bright regions become more visible after equalization.\n",
        "- **Facilitates Better Analysis**: For further image processing tasks such as segmentation or object detection, equalized images often yield better results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5YgUKGWudeY"
      },
      "source": [
        "\n",
        "### 1.6 Pre- and Post-Equalization Comparison\n",
        "A manual comparison is made between images before and after the application of histogram equalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "6Le90b-1seEl",
        "outputId": "f93bf9e4-f721-4d12-be3e-1468653b2515"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def histogram_equalization(image, bins=256):\n",
        "    \"\"\"Perform histogram equalization on a single image.\"\"\"\n",
        "    # Calculate the histogram\n",
        "    hist, bin_edges = np.histogram(image.flatten(), bins, density=False)\n",
        "    # Calculate the CDF\n",
        "    cdf = hist.cumsum()\n",
        "    cdf = 255 * cdf / cdf[-1]  # Normalize\n",
        "\n",
        "    # Use linear interpolation of the CDF to find new pixel values\n",
        "    image_equalized = np.interp(image.flatten(), bin_edges[:-1], cdf)\n",
        "    return image_equalized.reshape(image.shape)\n",
        "\n",
        "# We will perform this later\n",
        "# def resize_image(image, target_size=(640, 640), interpolation=cv2.INTER_LANCZOS4):\n",
        "#     \"\"\"Resize images to the target size and add a channel dimension if needed.\"\"\"\n",
        "#     resized_image = cv2.resize(image, target_size, interpolation=interpolation)\n",
        "#     if len(resized_image.shape) == 2:  # if only height and width, no channels\n",
        "#         resized_image = resized_image[:, :, np.newaxis]  # add a new axis for channels\n",
        "#     return resized_image\n",
        "\n",
        "def display_images(original_images, equalized_images):\n",
        "    \"\"\"Display original and equalized images side by side.\"\"\"\n",
        "    fig, axes = plt.subplots(2, len(original_images), figsize=(15, 5))\n",
        "\n",
        "    for i in range(len(original_images)):\n",
        "        axes[0, i].imshow(original_images[i], cmap='gray')\n",
        "        axes[0, i].set_title(f'Original Image {i+1}')\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        axes[1, i].imshow(equalized_images[i], cmap='gray')\n",
        "        axes[1, i].set_title(f'Equalized Image {i+1}')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Extract just the images from paired_data\n",
        "images = [img for img, _ in paired_data]\n",
        "\n",
        "# Equalize all images in the batch individually\n",
        "equalized_images = [histogram_equalization(img) for img in images]\n",
        "\n",
        "# Randomly select three images and their equalized counterparts for display\n",
        "random_indices = np.random.choice(len(images), min(3, len(images)), replace=False)\n",
        "selected_original_images = [images[i] for i in random_indices]\n",
        "selected_equalized_images = [equalized_images[i] for i in random_indices]\n",
        "\n",
        "display_images(selected_original_images, selected_equalized_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovJGDRsuiAms"
      },
      "source": [
        "Expected Output:\n",
        "> This displays a series of images sampled from the dataset. The top row is a set of original images which are dark and underexposed. The bottom shows equalized images that are brighter than the originals so that many features are more visible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7clDp1wdulTe"
      },
      "source": [
        "## 2. ROI Annotation Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d5ToppS66_z"
      },
      "source": [
        "### 2.1 ROI Category Consistency Check\n",
        "Text related to ROI formatting is examined to ensure consistent categorization across annotations. Here I check my assumptions about the format of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MVDw9NyvDRF",
        "outputId": "c9235b32-4e7e-4e04-b634-c5298aefe007"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_unique_values_for_roi_format(paired_data):\n",
        "    # Initialize a dictionary where keys are indices and values are sets of unique values at that index\n",
        "    unique_values = {}\n",
        "\n",
        "    # Iterate over paired data to populate unique values dictionary\n",
        "    for _, annotation in paired_data:\n",
        "        roi_format = annotation['information']['roi_format']\n",
        "        for idx, value in enumerate(roi_format):\n",
        "            if idx not in unique_values:\n",
        "                unique_values[idx] = set()\n",
        "            unique_values[idx].add(value)\n",
        "\n",
        "    return unique_values\n",
        "\n",
        "unique_values_dict = get_unique_values_for_roi_format(paired_data)\n",
        "\n",
        "# Display unique values for each index\n",
        "for idx, unique_vals in unique_values_dict.items():\n",
        "    print(f\"Index {idx} has unique values: {', '.join(unique_vals)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpI-KmFxiUJ5"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "Index 0 has unique values: type\n",
        "Index 1 has unique values: left\n",
        "Index 2 has unique values: top\n",
        "Index 3 has unique values: width\n",
        "Index 4 has unique values: height\n",
        "Index 5 has unique values: class\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1lfA3--fgL"
      },
      "source": [
        "### 2.2 Initial ROI Annotation Conversion\n",
        "Conversion from the JSON structure to a list of bounding box dimensions is performed for each ROI annotation. We assume that the tracing of annotations starts at the top left corner of a box, and then draws the box to the bottom right corner. If this is the case, then we assume that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ls2BFOyvOCt"
      },
      "outputs": [],
      "source": [
        "def reformat_annotations(pairs):\n",
        "    reformatted_pairs = []\n",
        "\n",
        "    for image, annotation in pairs:\n",
        "        rois = annotation['rois']\n",
        "        roi_format = annotation['information']['roi_format']\n",
        "\n",
        "        # Initialize a dictionary with keys from roi_format and empty list as values\n",
        "        formatted_annotation = {key: [] for key in roi_format}\n",
        "\n",
        "        # Fill the lists with corresponding roi values\n",
        "        for roi in rois:\n",
        "            for i, key in enumerate(roi_format):\n",
        "                formatted_annotation[key].append(roi[i])\n",
        "\n",
        "        # Add the reformatted annotation along with its image to the new list\n",
        "        reformatted_pairs.append((image, formatted_annotation))\n",
        "\n",
        "    return reformatted_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOn3utEQvLLi"
      },
      "source": [
        "### 2.3 Bounding Box Validity Check\n",
        "Bounding boxes that fall outside the known image dimensions are flagged and removed to clean the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhOlLnLopZ-h",
        "outputId": "92eadc1b-168e-4478-9310-3e4b6f414ebb"
      },
      "outputs": [],
      "source": [
        "def reformat_clean_annotations(pairs, clean=False):\n",
        "    reformatted_pairs = []\n",
        "\n",
        "    for index, (image, annotation) in enumerate(pairs):\n",
        "        rois = annotation['rois']\n",
        "        roi_format = annotation['information']['roi_format']\n",
        "        image_height, image_width = image.shape[:2]  # Where 'image' is a NumPy array\n",
        "\n",
        "        # We will only use the keys that are related to the ROI dimensions\n",
        "        keys = ['type', 'left', 'top', 'width', 'height']\n",
        "        formatted_annotation = {key: [] for key in keys}\n",
        "\n",
        "        # Check each annotation for validity\n",
        "        for i, roi in enumerate(rois):\n",
        "            # Extract ROI data, ignoring the 'class' key if it's present.\n",
        "            # We only have one class in our data so we can leave this off until\n",
        "            # we set up the tensor data for the model.\n",
        "            type_, left, top, width, height = roi[:5]\n",
        "\n",
        "            if clean:\n",
        "              # Check if the annotation is impossible\n",
        "              # In theory we assume that annotations that are less than the\n",
        "              # zero, zero cordinates of the top left are out of bounds. We also\n",
        "              # assume that our box width and height can not exceed the size of\n",
        "              # the image.\n",
        "              if left < 0 or left >= image_width or top < 0 or top >= image_height:\n",
        "                  continue  # Skip the impossible annotation\n",
        "\n",
        "            # Append the valid ROI data to the formatted_annotation\n",
        "            for j, key in enumerate(keys):\n",
        "                formatted_annotation[key].append(roi[j])\n",
        "\n",
        "        # Add the filtered and reformatted annotations to the list\n",
        "        reformatted_pairs.append((image, formatted_annotation))\n",
        "\n",
        "    return reformatted_pairs\n",
        "\n",
        "\n",
        "# Use the function on cleaned_pairs\n",
        "equalized_pairs = [(equalized_images[i], annotation) for i, (_, annotation) in enumerate(paired_data)]\n",
        "temp_pairs = reformat_annotations(equalized_pairs)\n",
        "cleaned_pairs = reformat_clean_annotations(equalized_pairs)\n",
        "\n",
        "print(f\"Length of first pair without removing out-of-range values: {len(temp_pairs[0][1]['left'])}\")\n",
        "print(f\"Length of first pair with out-of-range values removed: {len(cleaned_pairs[0][1]['left'])}\")\n",
        "\n",
        "print(f\"First pair without removing out-of-range values: {temp_pairs[0][1]}\")\n",
        "print(f\"First pair with out-of-range values removed: {cleaned_pairs[0][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "askQlMI6ibm9"
      },
      "source": [
        "Expected Output\n",
        "> In this case nothing is changed, we are just checking what is present in the data and seeing the impact of our function.\n",
        "\n",
        "```\n",
        "Length of first pair without removing out-of-range values: 12\n",
        "Length of first pair with out-of-range values removed: 12\n",
        "First pair without removing out-of-range values: {'type': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'left': [417, 272, 484, 311, 243, 442, 503, 498, 291, 95, 168, 172], 'top': [415, 423, -1, 117, 288, 478, 246, 95, 194, 98, 139, 447], 'width': [14, 12, 16, 11, 17, 12, 8, 12, 16, 15, 13, 14], 'height': [11, 12, 10, 11, 10, 16, 13, 21, 26, 12, 20, 22]}\n",
        "First pair with out-of-range values removed: {'type': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'left': [417, 272, 484, 311, 243, 442, 503, 498, 291, 95, 168, 172], 'top': [415, 423, -1, 117, 288, 478, 246, 95, 194, 98, 139, 447], 'width': [14, 12, 16, 11, 17, 12, 8, 12, 16, 15, 13, 14], 'height': [11, 12, 10, 11, 10, 16, 13, 21, 26, 12, 20, 22]}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-mCo_AFyaEM"
      },
      "source": [
        "### 2.4 ROI 'Class' Assumptions and Corrections\n",
        "The 'class' metric for ROI annotations is standardized, assuming all unlabelled objects are 'cells'. Here we discover that some annotations of the boxes are missing. In this case it becomes clear that some boxes are missing the \"cell\" label, so we fill in missing data with the label \"cell\" for each box drawn. There are no other label categories for the class, so this is only done for completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd8OtvXwwK17",
        "outputId": "4c03dbae-6e46-40c4-bbd6-96213e50e4ab"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# isolate annotations\n",
        "annotations = [annotation for _, annotation in temp_pairs]\n",
        "\n",
        "# Create DataFrame\n",
        "df_annotations = pd.DataFrame(annotations)\n",
        "\n",
        "print(\"roi_formats with missing values\")\n",
        "# Check for roi_formats with missing values across all columns\n",
        "print(df_annotations.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benQ7kx6jA8G"
      },
      "source": [
        "Expected Outputs:\n",
        "\n",
        "```\n",
        "roi_formats with missing values\n",
        "type       0\n",
        "left       0\n",
        "top        0\n",
        "width      0\n",
        "height     0\n",
        "class     39\n",
        "dtype: int64\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOOj5XVVwDTd",
        "outputId": "87cef7ba-712e-460d-d69b-fdbc4dee1644"
      },
      "outputs": [],
      "source": [
        "print(\"unique values for the 'class' roi_format\")\n",
        "\n",
        "# Replace NaN with empty lists\n",
        "df_annotations['class'] = df_annotations['class'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "# Flatten all lists and remove empty fields\n",
        "all_classes = [element for sublist in df_annotations['class'] if sublist for element in sublist]\n",
        "\n",
        "# Get the unique values\n",
        "unique_classes = list(set(all_classes))\n",
        "\n",
        "# Print the list of unique classes. There was definitely an easier way to do\n",
        "# this: (append all the class lists to a single list and then convert to set).\n",
        "print(unique_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz_iFfDwjIuw"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "unique values for the 'class' roi_format\n",
        "['cell']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyW16fVrwSRv"
      },
      "source": [
        "### 2.5 ROI Format Conversion and Validation\n",
        "ROI metrics are converted from the JSON format to the [ymin, xmin, ymax, xmax] format used by the model, and validated through manual inspection. The following link is useful for looking at various annotation methods, and technically the annotation method I adopt below is an inverted \"albumentations\" method, where the x and y coordinates are switched, however we still use normalized mins and maxes.\n",
        "\n",
        "https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg4GQHd1wv0Z",
        "outputId": "9676b565-be70-4e75-d722-37a7a3d273f3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def convert_rois_to_cartesian(roi_pairs):\n",
        "    image_array, roi_data = roi_pairs\n",
        "    image_height, image_width, _ = image_array.shape\n",
        "\n",
        "    # Create a list to hold the coordinates for bounding boxes\n",
        "    box_coordinates = []\n",
        "    for i in range(len(roi_data['top'])):\n",
        "      # Convert to [y_min, x_min, y_max, x_max] and normalize\n",
        "      y_min = roi_data['top'][i] / image_height\n",
        "      x_min = roi_data['left'][i] / image_width\n",
        "      y_max = (roi_data['top'][i] + roi_data['height'][i]) / image_height\n",
        "      x_max = (roi_data['left'][i] + roi_data['width'][i]) / image_width\n",
        "      # Append the box coordinates as a list\n",
        "      box_coordinates.append([y_min, x_min, y_max, x_max])\n",
        "\n",
        "    # Convert the list of box coordinates to a single numpy array\n",
        "    # print(box_coordinates)\n",
        "    numpy_boxes = np.array(box_coordinates, dtype=np.float32)\n",
        "    # Return a new tuple with the image array and the numpy array of box coordinates\n",
        "    return (image_array, numpy_boxes)\n",
        "\n",
        "cartesian_pair = convert_rois_to_cartesian(cleaned_pairs[0])\n",
        "cartesian_pair[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG3daeJWjZ3r"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "array([[ 0.8105469 ,  0.8144531 ,  0.83203125,  0.8417969 ],\n",
        "       [ 0.8261719 ,  0.53125   ,  0.8496094 ,  0.5546875 ],\n",
        "       [-0.00195312,  0.9453125 ,  0.01757812,  0.9765625 ],\n",
        "       [ 0.22851562,  0.6074219 ,  0.25      ,  0.62890625],\n",
        "       [ 0.5625    ,  0.47460938,  0.58203125,  0.5078125 ],\n",
        "       [ 0.93359375,  0.86328125,  0.96484375,  0.88671875],\n",
        "       [ 0.48046875,  0.9824219 ,  0.5058594 ,  0.9980469 ],\n",
        "       [ 0.18554688,  0.97265625,  0.2265625 ,  0.99609375],\n",
        "       [ 0.37890625,  0.5683594 ,  0.4296875 ,  0.5996094 ],\n",
        "       [ 0.19140625,  0.18554688,  0.21484375,  0.21484375],\n",
        "       [ 0.27148438,  0.328125  ,  0.31054688,  0.35351562],\n",
        "       [ 0.8730469 ,  0.3359375 ,  0.9160156 ,  0.36328125]],\n",
        "      dtype=float32)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW08B5-TIkKH",
        "outputId": "b5c0bd5f-2955-4969-9ab2-667f2f145b79"
      },
      "outputs": [],
      "source": [
        "cleaned_pairs[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFwMwHl8jef8"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "\n",
        "```\n",
        "(512, 512, 1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v44nUa0Gj-7z",
        "outputId": "adac9f98-49a2-49f0-cf67-fcf61144c91c"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "def draw_boxes_on_image(image_array, boxes, is_converted=False):\n",
        "    # Image preprocessing to ensure it's in the correct format\n",
        "    if image_array.dtype != np.uint8:\n",
        "        image_array = (255 * (image_array - image_array.min()) / (image_array.max() - image_array.min())).astype(np.uint8)\n",
        "    if image_array.shape[-1] == 1:\n",
        "        image_array = image_array.squeeze(-1)\n",
        "    if image_array.ndim == 2:\n",
        "        image = Image.fromarray(image_array, 'L').convert('RGB')\n",
        "    else:\n",
        "        image = Image.fromarray(image_array)\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "    # Define the loop range based on whether boxes are converted or not\n",
        "    loop_range = len(boxes) if is_converted else len(boxes['type'])\n",
        "\n",
        "    for index in range(loop_range):\n",
        "        # Determine the coordinates based on whether the boxes are converted\n",
        "        if is_converted:\n",
        "            box = boxes[index]  # Define the box variable here for the converted\n",
        "            # case.\n",
        "            # please note that because of the way the draw function expects\n",
        "            # inputs, we need to modify the order of our box array to ensure\n",
        "            # correct plotting.\n",
        "            left, top, right, bottom = box[1] * image.width, box[0] * image.height, box[3] * image.width, box[2] * image.height\n",
        "        else:\n",
        "            left, top, width, height = boxes['left'][index], boxes['top'][index], boxes['width'][index], boxes['height'][index]\n",
        "            right, bottom = left + width, top + height\n",
        "\n",
        "        # Draw the rectangle with green color\n",
        "        draw.rectangle([left, top, right, bottom], outline=(0, 255, 0), width=2)\n",
        "\n",
        "        # Draw the index number above the box\n",
        "        text_position = (left, top - 10 - 2)\n",
        "        draw.text(text_position, str(index), fill=(0, 255, 0), font=font)\n",
        "\n",
        "    return image\n",
        "\n",
        "def validate_coordinate_conversion(roi_pair, cartesian_pair):\n",
        "    # Extract the image and original ROIs from roi_pair\n",
        "    image_array, rois = roi_pair\n",
        "\n",
        "    # Extract the converted ROIs\n",
        "    _, numpy_boxes = cartesian_pair\n",
        "\n",
        "    # Draw the original ROIs on the image\n",
        "    image_with_original_rois = draw_boxes_on_image(np.copy(image_array), rois, is_converted=False)\n",
        "\n",
        "    # Draw the converted ROIs on the image\n",
        "    image_with_converted_rois = draw_boxes_on_image(np.copy(image_array), numpy_boxes, is_converted=True)\n",
        "\n",
        "    # Now, let's display both images side by side using matplotlib for comparison\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Convert the PIL images back to numpy arrays for displaying\n",
        "    image_with_original_rois_np = np.array(image_with_original_rois)\n",
        "    image_with_converted_rois_np = np.array(image_with_converted_rois)\n",
        "\n",
        "    # Display the images without cmap='gray' to ensure the green boxes are visible\n",
        "    ax1.imshow(image_with_original_rois_np)\n",
        "    ax1.set_title('Original ROI Annotation Format')\n",
        "\n",
        "    ax2.imshow(image_with_converted_rois_np)\n",
        "    ax2.set_title('Converted ROI Annotation Format')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Now we can compare the original and converted rois visually\n",
        "validate_coordinate_conversion(cleaned_pairs[0], cartesian_pair)\n",
        "cleaned_pairs[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMoVuBFhjm_6"
      },
      "source": [
        "Expected Output:\n",
        "> This displays the same image twice, with annotation boxes plotted on each image. One image uses boxes that used the original annotation format as input. The other image uses boxes plotted using the new annotation format as input. The outcome is that the boxes are in the same locations on both images--ensuring that the conversion of annotation formats did not alter or skew the geometry of the boxes. This helps make sure we are using an annotation method that is compatible with our machine learning model. This allows us to avoid having to retrain the annotation head of the model, saving us time and money.\n",
        "\n",
        "```\n",
        "{'type': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
        " 'left': [417, 272, 484, 311, 243, 442, 503, 498, 291, 95, 168, 172],\n",
        " 'top': [415, 423, -1, 117, 288, 478, 246, 95, 194, 98, 139, 447],\n",
        " 'width': [14, 12, 16, 11, 17, 12, 8, 12, 16, 15, 13, 14],\n",
        " 'height': [11, 12, 10, 11, 10, 16, 13, 21, 26, 12, 20, 22]}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiRyqg9dm7ei"
      },
      "source": [
        "Seeing that converting from ROI to cartesian coordinates still results in the same plotted bounding boxes, we move forward and convert the rest of the annotations. We randomly sample an index to double check quality. We then convert our image back to RGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgSv9m8plEF8",
        "outputId": "0628778c-92ae-4bc7-f04b-76d8bd02185f"
      },
      "outputs": [],
      "source": [
        "cartesian_pairs = [convert_rois_to_cartesian(pair) for pair in cleaned_pairs]\n",
        "cartesian_pairs[50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm4-jd7Gkwlm"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "\n",
        "```\n",
        "(array([[[  6.43861771],\n",
        "         [  2.35977374],\n",
        "         [  0.78339438],\n",
        "         ...,\n",
        "         [242.6128975 ],\n",
        "         [243.57959862],\n",
        "         [222.61333466]],\n",
        "\n",
        "        [[ 16.1225616 ],\n",
        "         [  6.43861771],\n",
        "         [  1.04764938],\n",
        "         ...,\n",
        "         [168.91843805],\n",
        "         [232.33611514],\n",
        "         [110.85475998]],\n",
        "\n",
        "        [[ 47.35530853],\n",
        "         [ 43.20379689],\n",
        "         [ 26.05213165],\n",
        "         ...,\n",
        "         [176.28017876],\n",
        "         [238.39163152],\n",
        "         [110.85475998]],\n",
        "\n",
        "        ...,\n",
        "\n",
        "        [[ 16.1225616 ],\n",
        "         [  6.43861771],\n",
        "         [  6.43861771],\n",
        "         ...,\n",
        "         [187.81331623],\n",
        "         [ 79.38983917],\n",
        "         [150.06483078]],\n",
        "\n",
        "        [[ 43.20379689],\n",
        "         [ 10.13463897],\n",
        "         [  5.01904138],\n",
        "         ...,\n",
        "         [150.06483078],\n",
        "         [239.98952866],\n",
        "         [246.46819522]],\n",
        "\n",
        "        [[224.44064874],\n",
        "         [ 76.22279718],\n",
        "         [ 10.13463897],\n",
        "         ...,\n",
        "         [ 43.20379689],\n",
        "         [196.8074304 ],\n",
        "         [ 23.54496462]]]),\n",
        " array([[0.04296875, 0.8886719 , 0.08984375, 0.91796875],\n",
        "        [0.19140625, 0.84375   , 0.22265625, 0.8730469 ],\n",
        "        [0.03515625, 0.5       , 0.06835938, 0.5253906 ],\n",
        "        [0.7519531 , 0.671875  , 0.79296875, 0.70703125],\n",
        "        [0.06640625, 0.1796875 , 0.09570312, 0.20507812],\n",
        "        [0.9140625 , 0.41210938, 0.9433594 , 0.44140625],\n",
        "        [0.29882812, 0.51171875, 0.34179688, 0.55078125],\n",
        "        [0.03320312, 0.32617188, 0.08984375, 0.36523438],\n",
        "        [0.43359375, 0.1953125 , 0.45507812, 0.21679688],\n",
        "        [0.40625   , 0.5957031 , 0.44335938, 0.62890625],\n",
        "        [0.390625  , 0.36523438, 0.43359375, 0.40039062],\n",
        "        [0.24414062, 0.8105469 , 0.28125   , 0.8417969 ],\n",
        "        [0.7988281 , 0.25390625, 0.8339844 , 0.27734375],\n",
        "        [0.19726562, 0.33398438, 0.25195312, 0.359375  ],\n",
        "        [0.53515625, 0.12304688, 0.57421875, 0.1484375 ],\n",
        "        [0.53125   , 0.921875  , 0.5703125 , 0.9550781 ],\n",
        "        [0.171875  , 0.00976562, 0.20898438, 0.04296875],\n",
        "        [0.46289062, 0.20703125, 0.5253906 , 0.25      ],\n",
        "        [0.13476562, 0.38867188, 0.16796875, 0.41796875],\n",
        "        [0.0078125 , 0.8105469 , 0.05273438, 0.8417969 ],\n",
        "        [0.8417969 , 0.2421875 , 0.8828125 , 0.27929688],\n",
        "        [0.13476562, 0.484375  , 0.16601562, 0.5097656 ],\n",
        "        [0.82421875, 0.03710938, 0.8613281 , 0.07226562],\n",
        "        [0.40039062, 0.17578125, 0.43164062, 0.19726562],\n",
        "        [0.24023438, 0.171875  , 0.2734375 , 0.1953125 ],\n",
        "        [0.01171875, 0.9277344 , 0.05078125, 0.95703125],\n",
        "        [0.29101562, 0.5449219 , 0.3359375 , 0.5761719 ],\n",
        "        [0.31054688, 0.609375  , 0.34375   , 0.63671875],\n",
        "        [0.00390625, 0.23046875, 0.03320312, 0.25585938],\n",
        "        [0.32617188, 0.6699219 , 0.34960938, 0.6875    ],\n",
        "        [0.8847656 , 0.29882812, 0.9433594 , 0.32617188],\n",
        "        [0.56640625, 0.6152344 , 0.5996094 , 0.640625  ],\n",
        "        [0.31054688, 0.11523438, 0.36523438, 0.14453125],\n",
        "        [0.46875   , 0.35742188, 0.5175781 , 0.38671875],\n",
        "        [0.52734375, 0.04101562, 0.5625    , 0.07617188],\n",
        "        [0.6777344 , 0.4921875 , 0.71875   , 0.5292969 ]], dtype=float32))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XpTjBy0roQ2T"
      },
      "outputs": [],
      "source": [
        "def convert_monochrome_to_rgb(image_array):\n",
        "  if image_array.shape[-1] == 3:\n",
        "    print(\"Already RGB!\")\n",
        "    return image_array\n",
        "\n",
        "  # Check if the image is of type float and convert to uint8\n",
        "  if image_array.dtype != np.uint8:\n",
        "      image_array = (255 * (image_array - image_array.min()) / (image_array.max() - image_array.min())).astype(np.uint8)\n",
        "\n",
        "  # If the image array has a shape of (H, W, 1), convert it to (H, W)\n",
        "  if image_array.shape[-1] == 1:\n",
        "      image_array = image_array.squeeze(-1)\n",
        "\n",
        "  # Convert grayscale (H, W) to RGB (H, W, 3) by stacking the grayscale image along the new axis\n",
        "  if image_array.ndim == 2:\n",
        "      image_array = np.stack((image_array,)*3, axis=-1)\n",
        "\n",
        "  return image_array\n",
        "\n",
        "for i, (image_array, annotation) in enumerate(cartesian_pairs):\n",
        "    rgb_image_array = convert_monochrome_to_rgb(image_array)\n",
        "    cartesian_pairs[i] = (rgb_image_array, annotation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoFqoiy2xWn_"
      },
      "source": [
        "## 3. Model Preparation and Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVqfTe8h78WR"
      },
      "source": [
        "The following code is heavily modified from the original \"Eager Few Shot Object Detection Colab\" tutorial which is part of the Object Detection library of tensorflow. To highlight a few differences, that tutorial detects a single object within a an image, while I have adapted the use of this model to find multiple objects within each image. Additionally, I have changed the optimizer, added a validation metrics using IOU, and functionality such as training checkpoints, in addition to some tooling for evaluating model performance holistically.\n",
        "\n",
        "https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAoVzBHmx7NU"
      },
      "source": [
        "\n",
        "### 3.1 Cloning TensorFlow Model Zoo and Setup\n",
        "The TensorFlow model zoo is cloned, and the environment is set up using protobuf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8SzHYMLPGV8",
        "outputId": "31af7768-38e7-4f0e-aa96-4e140f1294e8"
      },
      "outputs": [],
      "source": [
        "!pip install -U --pre tensorflow==\"2.2.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfIZmt1d8Fyu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWLJeCJM8T-Z"
      },
      "outputs": [],
      "source": [
        "# The setup of TensorFlow Object Detection API and compilation of .proto files using protobuff\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGrPX3rT8VGW"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbYqCv8typyK"
      },
      "source": [
        "### 3.2 Define untility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSzi4mkp8aPH"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path.\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None,\n",
        "                    min_score_thresh=0.8):\n",
        "  \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "  Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "      and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "      this function assumes that the boxes to be plotted are groundtruth\n",
        "      boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "      category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "  \"\"\"\n",
        "  image_np_with_annotations = image_np.copy()\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_annotations,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=min_score_thresh)\n",
        "  if image_name:\n",
        "    plt.imsave(image_name, image_np_with_annotations)\n",
        "  else:\n",
        "    plt.imshow(image_np_with_annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWDPxgUfxsWB"
      },
      "source": [
        "### 3.3 Image Resizing for Model Input\n",
        "Images are resized to 640x640 using `cv2.INTER_LANCZOS4` interpolation to match the model input requirements. When it comes to image processing, upscaling an image is a common task that involves increasing the resolution of the image. `cv2.INTER_LANCZOS4` is one of the interpolation methods provided by OpenCV, and it is especially beneficial for upscaling while retaining detail. `cv2.INTER_LANCZOS4` refers to the Lanczos interpolation over 8x8 pixel neighborhood. It is one of the most sophisticated and high-quality resampling algorithms provided by OpenCV for resizing images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv9kehhHyMcj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def resize_image_and_boxes(image, boxes, target_size=(640, 640)):\n",
        "    '''Resize the image and the bounding boxes. This a assumes that box cordinates are already normalized.'''\n",
        "\n",
        "    # Resize image\n",
        "    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
        "\n",
        "    # Add channel dimension if missing\n",
        "    if len(resized_image.shape) == 2:\n",
        "        resized_image = resized_image[..., np.newaxis]\n",
        "\n",
        "    return resized_image, boxes\n",
        "\n",
        "# Resize the images and boxes to use the 640x640 model input shape\n",
        "resized_data = [resize_image_and_boxes(image, boxes) for image, boxes in cartesian_pairs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94G-zoVyI1y"
      },
      "source": [
        "### 3.4 Data Split into Training and Testing Sets\n",
        "The dataset is divided into 70% training data and 30% testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh1mzhc3yX2G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def train_validation_split(data, train_ratio=0.70):\n",
        "    \"\"\"\n",
        "    Splits the data into training and validation sets based on the specified training ratio.\n",
        "    \"\"\"\n",
        "    np.random.shuffle(data)\n",
        "    train_size = int(len(data) * train_ratio)\n",
        "    training_data = data[:train_size]\n",
        "    validation_data = data[train_size:]\n",
        "    return training_data, validation_data\n",
        "\n",
        "# Split the dataset\n",
        "training_data, validation_data = train_validation_split(resized_data)\n",
        "print(f\"Training data size: {len(training_data)}\")\n",
        "print(f\"Validation data size: {len(validation_data)}\")\n",
        "\n",
        "# Initialize lists to hold training and test data\n",
        "train_images_np, train_gt_boxes = [], []\n",
        "test_images_np, test_gt_boxes = [], []\n",
        "\n",
        "# Load the training images and annotations\n",
        "for images, annotations in training_data:\n",
        "    train_images_np.append(images)\n",
        "    train_gt_boxes.append(annotations)\n",
        "\n",
        "# Load the validation images and annotations\n",
        "for images, annotations in validation_data:\n",
        "    test_images_np.append(images)\n",
        "    test_gt_boxes.append(annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxeDuWzvyf8n"
      },
      "source": [
        "### 3.5 Manual Inspection of Formatted Images\n",
        "Formatted images undergo a manual inspection to ensure consistent preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRnSRHEr8fN-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "print(\"Shape of resized image tensors:\", train_images_np[0].shape)\n",
        "\n",
        "for idx, train_image_np in enumerate(train_images_np[0:6]):\n",
        "  plt.subplot(2, 3, idx+1)\n",
        "  plt.imshow(train_image_np)\n",
        "print(\"Sample of training images\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Sample of annotation data for the first image, aka 'ground truth boxes'\")\n",
        "print(train_gt_boxes[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLfLqeW_-7OF"
      },
      "source": [
        "### 3.6 Training Data Box Statistics Generation\n",
        "Statistics for training data bounding boxes are generated for reference against test data.\n",
        "\n",
        "#### Average Distance Between Boxes\n",
        "- The average Euclidean distance between the centers of all pairs of bounding boxes within each image. This statistic provides insight into the average spatial distribution of objects in an image.\n",
        "\n",
        "#### Count per Image\n",
        "- The total number of bounding boxes detected in each image. This count helps understand the average density of detectable objects per image.\n",
        "\n",
        "#### Size Statistics\n",
        "- `size_mean`: The mean size of bounding boxes, provided as (width, height). It indicates the average dimensions of the detected objects.\n",
        "- `size_median`: The median size of bounding boxes, which can be less sensitive to outliers than the mean.\n",
        "- `size_std`: The standard deviation of the sizes of bounding boxes, indicating the variability in the size of detected objects.\n",
        "\n",
        "#### Aspect Ratio Statistics\n",
        "- `aspect_ratio_mean`: The mean of the aspect ratios (height divided by width) of all bounding boxes, which gives an average shape of objects.\n",
        "- `aspect_ratio_median`: The median of the aspect ratios, again providing a measure less affected by outliers.\n",
        "- `aspect_ratio_std`: The standard deviation of the aspect ratios, showing how much the shape of detected objects varies.\n",
        "\n",
        "#### Position Statistics\n",
        "- `position_mean`: The mean position of bounding box centers, given as (y, x). This reflects the average location where objects tend to be centered in the image.\n",
        "- `position_std`: The standard deviation of the positions of bounding box centers, indicating how spread out the objects are in the image.\n",
        "\n",
        "#### Edge Proximity Statistics\n",
        "- `edge_proximity_mean`: The mean minimum distance of each bounding box to the nearest edge of the image. A lower number suggests objects are often closer to the edges.\n",
        "- `edge_proximity_std`: The standard deviation of the distances to the edges, showing the variability in how close objects tend to be to the edge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtPVKZwn9NPW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_statistics(train_gt_boxes):\n",
        "    # Assumes train_gt_boxes is a list of Numpy arrays with the shape (N, 4)\n",
        "    # Each sub-array contains bounding boxes in the form [ymin, xmin, ymax, xmax]\n",
        "    all_distances = []\n",
        "    all_sizes = []\n",
        "    all_aspect_ratios = []\n",
        "    all_positions = []\n",
        "    all_edge_proximities = []\n",
        "\n",
        "    for boxes in train_gt_boxes:\n",
        "        # Calculate center positions\n",
        "        centers = np.c_[ (boxes[:, 2] + boxes[:, 0]) / 2,\n",
        "                         (boxes[:, 3] + boxes[:, 1]) / 2 ]\n",
        "        all_positions.append(centers)\n",
        "\n",
        "        # Calculate sizes (width, height)\n",
        "        sizes = np.c_[ boxes[:, 3] - boxes[:, 1],\n",
        "                       boxes[:, 2] - boxes[:, 0] ]\n",
        "        all_sizes.append(sizes)\n",
        "\n",
        "        # Calculate aspect ratios\n",
        "        aspect_ratios = sizes[:, 1] / sizes[:, 0]\n",
        "        all_aspect_ratios.extend(aspect_ratios)\n",
        "\n",
        "        # Calculate edge proximity\n",
        "        edge_proximity = np.minimum.reduce([boxes[:, 1], boxes[:, 0], 1 - boxes[:, 3], 1 - boxes[:, 2]])\n",
        "        all_edge_proximities.extend(edge_proximity)\n",
        "\n",
        "        # Calculate distances between each pair of boxes\n",
        "        if len(centers) > 1:\n",
        "            distances = np.sqrt(np.sum((centers[:, np.newaxis, :] - centers[np.newaxis, :, :]) ** 2, axis=2))\n",
        "            # We only want the upper triangle without the diagonal, since the matrix is symmetric and the diagonal is 0\n",
        "            triu_indices = np.triu_indices_from(distances, k=1)\n",
        "            all_distances.extend(distances[triu_indices])\n",
        "\n",
        "    # Convert to Numpy arrays for easier manipulation\n",
        "    all_distances = np.array(all_distances)\n",
        "    all_sizes = np.concatenate(all_sizes, axis=0)\n",
        "    all_aspect_ratios = np.array(all_aspect_ratios)\n",
        "    all_edge_proximities = np.array(all_edge_proximities)\n",
        "\n",
        "    # Calculate the statistics\n",
        "    stats = {\n",
        "        'average_distance_between_boxes': np.mean(all_distances) if len(all_distances) else None,\n",
        "        'box_count_per_image': [len(boxes) for boxes in train_gt_boxes],\n",
        "        'box_size_mean': np.mean(all_sizes, axis=0),\n",
        "        'box_size_median': np.median(all_sizes, axis=0),\n",
        "        'box_size_std': np.std(all_sizes, axis=0),\n",
        "        'box_aspect_ratio_mean': np.mean(all_aspect_ratios),\n",
        "        'box_aspect_ratio_median': np.median(all_aspect_ratios),\n",
        "        'box_aspect_ratio_std': np.std(all_aspect_ratios),\n",
        "        'box_position_mean': np.mean(np.concatenate(all_positions, axis=0), axis=0),\n",
        "        'box_position_std': np.std(np.concatenate(all_positions, axis=0), axis=0),\n",
        "        'box_edge_proximity_mean': np.mean(all_edge_proximities),\n",
        "        'box_edge_proximity_std': np.std(all_edge_proximities)\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "def plot_count_histogram(counts):\n",
        "    '''Function to plot a histogram with counts per bin and number of occurrences per bin'''\n",
        "    # There's an issue with how this notebook is rendering numbers, likely\n",
        "    # because of some of the tensorflow visualization tools modifying HTML/CSS\n",
        "    # of the notebook. I included this plot as a general reference, but a production\n",
        "    # implementation should be able to render axis numbers correctly.\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    n, bins, patches = plt.hist(counts, bins=20, color='skyblue', edgecolor='black')\n",
        "\n",
        "    # Set the title and labels\n",
        "    plt.title('Count of Bounding Boxes per Image')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    # Set the ticks to be at the edges of the bins\n",
        "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
        "    plt.xticks(bin_centers, [f'{bin:.2f}' for bin in bin_centers])\n",
        "    plt.yticks(range(int(max(n) + 1)))\n",
        "\n",
        "    # Show the plot with a grid\n",
        "    plt.show()\n",
        "\n",
        "print(\"Statistics based upon normalized coordinates.\")\n",
        "stats = calculate_statistics(train_gt_boxes)\n",
        "for key, value in stats.items():\n",
        "  print(f\"{key}: {value}\")\n",
        "plot_count_histogram(stats['box_count_per_image'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxhKrDedy7XI"
      },
      "source": [
        "## 4. Model Configuration and Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQbMgoRdy92u"
      },
      "source": [
        "### 4.1 Creation of Single Class Category Index\n",
        "A category index for the single class present in the images is created.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzBPTh0FzFdd"
      },
      "source": [
        "### 4.2 Training Data Conversion to Tensors\n",
        "The training images and ground truth boxes are converted into tensors with one-hot encoding applied to annotations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFjrOtpu8-C-"
      },
      "outputs": [],
      "source": [
        "# By convention, our non-background classes start counting at 1.  Given\n",
        "# that we will be predicting just one class, we will therefore assign it a\n",
        "# `class id` of 1.\n",
        "cell_class_id = 1\n",
        "category_index = {cell_class_id: {'id': cell_class_id, 'name': 'cell'}}\n",
        "num_classes = 1\n",
        "\n",
        "# Convert class labels to one-hot; convert everything to tensors.\n",
        "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
        "# we do this here so that the model receives one-hot labels where non-background\n",
        "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
        "# automatically in our training binaries, but we need to reproduce it here.\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "\n",
        "# lists containing the one-hot encoded classes and ground truth boxes\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "for (train_image_np, gt_box_np) in zip(train_images_np, train_gt_boxes):\n",
        "\n",
        "  # convert training image to tensor, add batch dimension, and add to list\n",
        "  train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "      train_image_np, dtype=tf.float32), axis=0))\n",
        "\n",
        "  # convert numpy array to tensor, then add to list\n",
        "  gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "\n",
        "  # apply offset to to have zero-indexed ground truth classes\n",
        "  zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "      np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "\n",
        "  # do one-hot encoding to ground truth classes\n",
        "  gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "      zero_indexed_groundtruth_classes, num_classes))\n",
        "print('Done prepping data.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Mk4NmpF8JQ"
      },
      "source": [
        "### 4.3 Annotation Plotting, Inspection, and Optional Augmentation\n",
        "Annotations are plotted for manual inspection to ensure correct object alignment. We are also visualizing the data in an un-augmented state with the option to later introduce augmentations after this section of code. In many cases we can see how the annotations are quite \"fuzzy\" and do not closely gate the object. We also see many objects that look like cells that are not annotated--with little distinction of why they were excluded. I believe this confounds the learning of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vDGFSh4-hkhy",
        "outputId": "6d212e70-7c14-44ca-b729-8b9b4b4211dd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 15))\n",
        "indices = np.random.choice(len(train_images_np), 6, replace=False)\n",
        "print(f\"Random indices selected: {indices}\")\n",
        "temp_images = [train_images_np[i] for i in indices]\n",
        "temp_train_boxes = [train_gt_boxes[i] for i in indices]\n",
        "\n",
        "# Now you can loop over the temporary lists to display the images\n",
        "for idx in range(len(temp_images)):\n",
        "    plt.subplot(2, 3, idx + 1)\n",
        "\n",
        "    # The number of boxes for the current image\n",
        "    num_boxes = temp_train_boxes[idx].shape[0]\n",
        "\n",
        "    # Create an array of ones with a length equal to the number of boxes\n",
        "    # This assumes each box is to be given a score of 1.0 (or 100%)\n",
        "    dummy_scores = np.ones(shape=[num_boxes], dtype=np.float32)\n",
        "\n",
        "    plot_detections(\n",
        "        temp_images[idx],\n",
        "        temp_train_boxes[idx],\n",
        "        np.ones(shape=[num_boxes], dtype=np.int32),  # class IDs for each box\n",
        "        dummy_scores,  # scores for each box\n",
        "        category_index)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGBsAk2blLnL"
      },
      "source": [
        "Expected Output:\n",
        "> This displays six images with annotations rendered for each image. The annotations are boxes over the cells in the images. Each box has a object label and a accuracy score above it. In this case everything says \"cell: 100%\"\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Random indices selected: [57 33  6 11  4 51]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8vXyqvh7a63"
      },
      "source": [
        "#### Optional Image Augmentation\n",
        "Here we define data augmentation methods and visualize augmented data to ensure bounding boxes align correctly (This code is a work in progress and not currently working. Part of the reason I didn't prioritize it is because of our input data quality adding significant issues to training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wplMiVUcGB4a"
      },
      "outputs": [],
      "source": [
        "# # Function to flip an image horizontally using TensorFlow\n",
        "# def tf_flip_image_horizontal(image):\n",
        "#     return tf.image.flip_left_right(image)\n",
        "\n",
        "# # Function to rotate an image by 90 degrees clockwise using TensorFlow\n",
        "# def tf_rotate_image_90(image):\n",
        "#     return tf.image.rot90(image, k=3)  # k=3 for 90 degrees clockwise rotation\n",
        "\n",
        "# # Function to adjust bounding boxes for horizontal flip using TensorFlow\n",
        "# def tf_adjust_boxes_for_flip(boxes, image_width):\n",
        "#     new_boxes = tf.stack([boxes[:, 0], image_width - boxes[:, 3],\n",
        "#                           boxes[:, 2], image_width - boxes[:, 1]], axis=1)\n",
        "#     return new_boxes\n",
        "\n",
        "# # Function to adjust bounding boxes for 90 degree rotation using TensorFlow\n",
        "# def tf_adjust_boxes_for_rotation(boxes, image_height):\n",
        "#     new_boxes = tf.stack([boxes[:, 1], image_height - boxes[:, 2],\n",
        "#                           boxes[:, 3], image_height - boxes[:, 0]], axis=1)\n",
        "#     return new_boxes\n",
        "\n",
        "# # Define a function to apply augmentations\n",
        "# def apply_augmentations(image, boxes):\n",
        "#     # Apply horizontal flip\n",
        "#     flipped_image = tf_flip_image_horizontal(image)\n",
        "#     flipped_boxes = tf_adjust_boxes_for_flip(boxes, image.shape[1])\n",
        "\n",
        "#     # Apply 90 degree rotation\n",
        "#     rotated_image = tf_rotate_image_90(flipped_image)\n",
        "#     rotated_boxes = tf_adjust_boxes_for_rotation(flipped_boxes, image.shape[0])\n",
        "\n",
        "#     return rotated_image, rotated_boxes\n",
        "\n",
        "# # Now augment and visualize some images\n",
        "# plt.figure(figsize=(30, 15))\n",
        "# indices = np.random.choice(len(train_images_np), 6, replace=False)\n",
        "\n",
        "# for idx, image_idx in enumerate(indices):\n",
        "#     image_np = train_images_np[image_idx]\n",
        "#     gt_boxes = train_gt_boxes[image_idx]\n",
        "\n",
        "#     # Apply augmentations\n",
        "#     augmented_image_np, augmented_boxes = apply_augmentations(image_np, gt_boxes)\n",
        "\n",
        "#     # Convert to numpy arrays if they are tensors\n",
        "#     if isinstance(augmented_image_np, tf.Tensor):\n",
        "#         augmented_image_np = augmented_image_np.numpy()\n",
        "#     if isinstance(augmented_boxes, tf.Tensor):\n",
        "#         augmented_boxes = augmented_boxes.numpy()\n",
        "\n",
        "#     # Normalize box coordinates\n",
        "#     image_height, image_width, _ = augmented_image_np.shape\n",
        "#     normalized_boxes = augmented_boxes.copy()\n",
        "#     normalized_boxes[:, [0, 2]] /= image_height  # y coordinates\n",
        "#     normalized_boxes[:, [1, 3]] /= image_width   # x coordinates\n",
        "\n",
        "#     # Check if the boxes are normalized between 0 and 1\n",
        "#     assert normalized_boxes.min() >= 0.0 and normalized_boxes.max() <= 1.0, \\\n",
        "#         \"Box coordinates are not normalized properly.\"\n",
        "\n",
        "#     plt.subplot(2, 3, idx + 1)\n",
        "\n",
        "#     num_boxes = len(normalized_boxes)\n",
        "#     dummy_scores = np.ones(shape=[num_boxes], dtype=np.float32)  # Dummy scores for visualization\n",
        "\n",
        "#     # Visualize the augmented image and boxes\n",
        "#     plot_detections(\n",
        "#         augmented_image_np,\n",
        "#         normalized_boxes,\n",
        "#         np.ones(shape=[num_boxes], dtype=np.int32),  # class IDs for each box\n",
        "#         dummy_scores,  # scores for each box\n",
        "#         category_index)\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV5QDIK6za06"
      },
      "source": [
        "### 4.4 Model Checkpoint Download and Configuration\n",
        "The `sd_resnet50_v1_fpn_640x640_coco17_tpu-8` checkpoint is downloaded, and the model configuration file is modified.\n",
        "#### SSD with ResNet-50 and FPN Model Overview\n",
        "\n",
        "The `ssd_resnet50_v1_fpn_640x640_coco17_tpu-8` configuration defines an object detection model that synergizes various powerful components to produce an efficient and scalable architecture. This text delves into the individual components of the model and explicates the unique benefits they confer to the object detection domain.\n",
        "\n",
        "#### Model Architecture Components\n",
        "\n",
        "##### SSD (Single Shot Multibox Detector):\n",
        "- Employs a singular neural network to accomplish the dual tasks of object localization and classification in one forward pass, thus enhancing processing speed.\n",
        "\n",
        "##### ResNet-50 Backbone:\n",
        "- Acts as the foundational feature extraction component of the model.\n",
        "- ResNet-50 employs a series of 'residual connections' which effectively allow layers to skip over one another, facilitating the training of deeper networks by circumventing the vanishing gradient dilemma.\n",
        "\n",
        "##### FPN (Feature Pyramid Network):\n",
        "- Enhances the SSD framework by constructing a 'multi-scale feature hierarchy' wherein each pyramid level represents a distinct scale, bolstering the detection of objects of disparate sizes.\n",
        "\n",
        "##### Input Size (640x640):\n",
        "- The model accepts input images of 640x640 pixels, offering a compromise between capturing image detail and computational tractability.Given our 512x512 image dimensions, this is a good match for our needs.\n",
        "\n",
        "##### COCO Dataset:\n",
        "- The COCO dataset encompasses a diverse array of object categories, serving as one of the benchmarks in object detection.\n",
        "\n",
        "##### Optimized for TPU-8:\n",
        "- Configuration is specialized for optimal performance on Google's Tensor Processing Units (TPUs) with 8 cores, enabling swift training and inference.\n",
        "\n",
        "#### Benefits of the Model\n",
        "\n",
        "##### Speed:\n",
        "- Intrinsic to the SSD architecture, speed is a hallmark, rendering the model apt for real-time detection scenarios.\n",
        "\n",
        "##### Accuracy:\n",
        "- ResNet-50's track record of accurate feature extraction is well-established.\n",
        "- FPN contributes significantly to the model's precision by improving object detection across scales.\n",
        "\n",
        "##### Scalability:\n",
        "- TPU compatibility ensures the model scales efficiently, facilitating the processing of expansive datasets.\n",
        "\n",
        "##### Versatility:\n",
        "- The model's proficiency in identifying a broad spectrum of objects is fortified by its comprehensive training on the COCO dataset.\n",
        "\n",
        "##### Balance of Performance and Resources:\n",
        "- The chosen input size and the inherent efficiency of the SSD method allow for an optimal balance between performance and computational resource expenditure.\n",
        "\n",
        "#### Detailed Architecture of ResNet-50\n",
        "\n",
        "ResNet-50 is a variant of the Residual Network architecture that contains 50 layers. The key innovation in ResNet is the introduction of 'residual connections' which skip one or more layers. Traditional neural networks might suffer from the vanishing gradient problem as they become deeper, making training very deep networks challenging. Residual connections combat this by allowing gradients to flow through the network more easily. Each residual block in a ResNet contains two paths: one where the input is passed through weights (and non-linearities), and another where the input bypasses this transformation and is added to the output of the weighted path. This architecture encourages the network to learn residual functions, which are modifications to the identity mapping, rather than learning unreferenced functions each time.\n",
        "\n",
        "#### Understanding Multi-Scale Feature Hierarchy\n",
        "\n",
        "A multi-scale feature hierarchy within the FPN constructs a pyramidal structure where each level of the pyramid corresponds to features extracted at a different scale. This setup enables the model to detect objects at various sizes. In practice, lower levels of the pyramid have higher resolution features which are good for detecting smaller objects, while higher levels have coarser features suitable for identifying larger objects. This multi-scale approach is crucial for handling the inherent scale variability of objects within real-world images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ADy2GCqA9liC",
        "outputId": "27d7f9d2-77bb-42ac-90bc-8933cf2fddf3"
      },
      "outputs": [],
      "source": [
        "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi3ntlOendrK"
      },
      "source": [
        "Expected Output:\n",
        "\n",
        "\n",
        "```\n",
        "--2023-11-09 19:39:49--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.12.207, 172.217.194.207, 172.253.118.207, ...\n",
        "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.12.207|:80... connected.\n",
        "HTTP request sent, awaiting response... 200 OK\n",
        "Length: 244817203 (233M) [application/x-tar]\n",
        "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz.1’\n",
        "\n",
        "ssd_resnet50_v1_fpn 100%[===================>] 233.48M  20.9MB/s    in 12s     \n",
        "\n",
        "2023-11-09 19:40:01 (19.5 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz.1’ saved [244817203/244817203]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6We4AVRze9L"
      },
      "source": [
        "### 4.5 Model Weight Initialization and Freezing\n",
        "The model is initialized with fake data to prepare for training, with weights frozen except for the heads to enable fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ugBz3kn9nrK"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
        "num_classes = 1\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Load pipeline config and build a detection model.\n",
        "#\n",
        "# Since we are working off of a COCO architecture which predicts 90\n",
        "# class slots by default, we override the `num_classes` field here to be just\n",
        "# one (for our new cell class).\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "model_config.ssd.num_classes = num_classes\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "\n",
        "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
        "# `heads` --- one for classification, the other for box regression.  We will\n",
        "# restore the box regression head but initialize the classification head\n",
        "# from scratch (we show the omission below by commenting out the line that\n",
        "# we would add if we wanted to restore both heads)\n",
        "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "    #    (i.e., the classification head that we *will not* restore)\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "fake_model = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=fake_box_predictor)\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "# Run model through a dummy image so that variables are created\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)\n",
        "print('Weights restored!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPd7J5aQzrl1"
      },
      "source": [
        "## 5. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJjlDmH2aMn2"
      },
      "outputs": [],
      "source": [
        "def compute_iou(groundtruth_boxes, detection_boxes):\n",
        "    \"\"\"\n",
        "    Computes the IoU between ground truth and detection boxes.\n",
        "\n",
        "    Args:\n",
        "        groundtruth_boxes: a Tensor of shape [num_gt_boxes, 4]\n",
        "        detection_boxes: a Tensor of shape [1, num_detections, 4]\n",
        "    \"\"\"\n",
        "    # Expand the ground truth boxes tensor to match the shape of detection boxes\n",
        "    gt_boxes = tf.expand_dims(groundtruth_boxes, axis=1)  # Now [num_gt_boxes, 1, 4]\n",
        "    # Remove the batch dimension from detection boxes since it's not needed here\n",
        "    detection_boxes = tf.squeeze(detection_boxes, axis=0)  # Now [num_detections, 4]\n",
        "\n",
        "    # Calculate intersection areas\n",
        "    gt_ymin, gt_xmin, gt_ymax, gt_xmax = tf.split(gt_boxes, 4, axis=2)\n",
        "    d_ymin, d_xmin, d_ymax, d_xmax = tf.split(detection_boxes, 4, axis=1)\n",
        "\n",
        "    inter_xmin = tf.maximum(gt_xmin, d_xmin)\n",
        "    inter_ymin = tf.maximum(gt_ymin, d_ymin)\n",
        "    inter_xmax = tf.minimum(gt_xmax, d_xmax)\n",
        "    inter_ymax = tf.minimum(gt_ymax, d_ymax)\n",
        "\n",
        "    inter_area = tf.maximum(inter_xmax - inter_xmin, 0) * tf.maximum(inter_ymax - inter_ymin, 0)\n",
        "\n",
        "    # Calculate union areas\n",
        "    gt_area = (gt_xmax - gt_xmin) * (gt_ymax - gt_ymin)\n",
        "    d_area = (d_xmax - d_xmin) * (d_ymax - d_ymin)\n",
        "\n",
        "    union_area = gt_area + d_area - inter_area\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = inter_area / union_area\n",
        "\n",
        "    # Squeeze to remove the extra dimension\n",
        "    iou = tf.squeeze(iou, axis=2)\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "def validate_model_on_batch(detection_model, validation_data, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Validate the model on a batch of validation data.\n",
        "    \"\"\"\n",
        "    iou_scores = []\n",
        "    for image_tensor, gt_boxes_tensor in validation_data:\n",
        "        preprocessed_image, shapes = detection_model.preprocess(image_tensor)\n",
        "        prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "        detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "        # Compute IoU\n",
        "        iou = compute_iou(gt_boxes_tensor, detections['detection_boxes'])\n",
        "\n",
        "        # Calculate the accuracy based on IoU threshold\n",
        "        correct_predictions = tf.cast(iou >= iou_threshold, tf.float32)\n",
        "        accuracy = tf.reduce_mean(correct_predictions)\n",
        "\n",
        "        iou_scores.append(accuracy)\n",
        "\n",
        "    # Compute the mean IoU score across all validation data\n",
        "    mean_iou = tf.reduce_mean(iou_scores)\n",
        "\n",
        "    return mean_iou.numpy()  # Return the mean IoU as a numpy float\n",
        "\n",
        "def prepare_validation_tensors(images, boxes):\n",
        "    image_tensors = []\n",
        "    box_tensors = []\n",
        "    for image_np, box_np in zip(images, boxes):\n",
        "        # Convert to tensor and check if the batch dimension is already present\n",
        "        image_tensor = tf.convert_to_tensor(image_np, dtype=tf.float32)\n",
        "        box_tensor = tf.convert_to_tensor(box_np, dtype=tf.float32)\n",
        "        if image_tensor.shape[0] != 1:\n",
        "            image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
        "        image_tensors.append(image_tensor)\n",
        "        box_tensors.append(box_tensor)\n",
        "    return image_tensors, box_tensors\n",
        "\n",
        "# Set up forward + backward pass for a single train step.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "  \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "  # Use tf.function for a bit of speed.\n",
        "  # Comment out the tf.function decorator if you want the inside of the\n",
        "  # function to run eagerly.\n",
        "  #@tf.function\n",
        "  def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "    with tf.GradientTape() as tape:\n",
        "      preprocessed_images = tf.concat(\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "      losses_dict = model.loss(prediction_dict, shapes)\n",
        "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "    return total_loss\n",
        "\n",
        "  return train_step_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0fnEYzPztlS"
      },
      "source": [
        "### 5.1 Test Data Tensor Preparation\n",
        "Test data is loaded as tensors to provide validation metrics during model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q38gXwZV_VkR"
      },
      "source": [
        "### 5.2 Loading Pre-existing Checkpoints\n",
        "Checkpoints are loaded if available to continue training from a previous state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tox6PsCEz63s"
      },
      "source": [
        "### 5.3 Model Training and Performance Metrics\n",
        "The training process includes forward and backward passes with calculation of loss and validation accuracy to monitor performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p37TDqYM9yOo"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# Select variables in top layers to fine-tune.\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "# Use the function to prepare the tensors\n",
        "test_image_tensors, test_gt_box_tensors = prepare_validation_tensors(test_images_np, test_gt_boxes)\n",
        "validation_tensors = list(zip(test_image_tensors, test_gt_box_tensors))\n",
        "\n",
        "checkpoint_dir = './checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint = tf.train.Checkpoint(model=detection_model, optimizer=optimizer)\n",
        "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=3)\n",
        "\n",
        "# Attempt to restore from the latest checkpoint\n",
        "status = checkpoint.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "else:\n",
        "    print(\"Initializing from scratch.\")\n",
        "\n",
        "# These parameters can be tuned;\n",
        "batch_size = 4\n",
        "learning_rate = 0.01\n",
        "num_batches = 100\n",
        "iou_threshold = 0.75\n",
        "\n",
        "print('Start fine-tuning!', flush=True)\n",
        "for idx in range(num_batches):\n",
        "  # Grab keys for a random subset of examples\n",
        "  all_keys = list(range(len(train_images_np)))\n",
        "  random.shuffle(all_keys)\n",
        "  example_keys = all_keys[:batch_size]\n",
        "\n",
        "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "  # Training step (forward pass + backwards pass)\n",
        "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "  if idx % 10 == 0:\n",
        "    total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "    print(f'Batch {idx} of {num_batches}, loss={total_loss.numpy():.4f}', flush=True)\n",
        "  if idx % 50 == 0:\n",
        "          save_path = manager.save()\n",
        "          print(\"Saved checkpoint for batch {}: {}\".format(idx, save_path))\n",
        "          validation_accuracy = validate_model_on_batch(detection_model, validation_tensors, iou_threshold)\n",
        "          print(f'Batch {idx}, Validation Accuracy: {validation_accuracy:.2%}', flush=True)\n",
        "\n",
        "# Perform a final validation step after training is complete\n",
        "print('Final validation on test data...', flush=True)\n",
        "validation_accuracy = validate_model_on_batch(detection_model, validation_tensors, iou_threshold)\n",
        "print(f'Final Validation Accuracy: {validation_accuracy:.2%}')\n",
        "\n",
        "# Save the final checkpoint after training\n",
        "save_path = manager.save()\n",
        "print(f\"Saved final checkpoint: {save_path}\")\n",
        "\n",
        "print('Done fine-tuning!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7atUjs2iz-uB"
      },
      "source": [
        "\n",
        "### 5.4 Model Detection Inspection\n",
        "Detections made by the model are manually plotted and inspected for accuracy and precision to assess the model's real-world performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR8SVxOYIZQ0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_predictions_and_ground_truth(image, gt_boxes, pred_boxes, pred_scores, threshold=0.35):\n",
        "    \"\"\"\n",
        "    Plots the unannotated and annotated images side by side.\n",
        "\n",
        "    Args:\n",
        "    image (ndarray): The image on which to plot the boxes.\n",
        "    gt_boxes (ndarray): The ground truth boxes.\n",
        "    pred_boxes (ndarray): The predicted boxes.\n",
        "    pred_scores (ndarray): The confidence scores for the predicted boxes.\n",
        "    threshold (float): The score threshold to consider for plotting predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize image if necessary\n",
        "    if image.dtype == np.float32 and image.max() > 1.0:\n",
        "        image /= 255.0\n",
        "    elif image.dtype == np.uint8:\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9), sharex=True, sharey=True)\n",
        "\n",
        "    # Display the unannotated image on the first subplot\n",
        "    ax1.imshow(image)\n",
        "    ax1.set_title('Unannotated Image')\n",
        "    ax1.axis('off')  # Turn off the axis\n",
        "\n",
        "    # Display the annotated image on the second subplot\n",
        "    ax2.imshow(image)\n",
        "    ax2.set_title('Annotated Image')\n",
        "\n",
        "    # Plot ground truth boxes and predicted boxes on the annotated image\n",
        "    for box in gt_boxes:\n",
        "        ymin, xmin, ymax, xmax = box\n",
        "        rect = patches.Rectangle((xmin*image.shape[1], ymin*image.shape[0]),\n",
        "                                 (xmax-xmin)*image.shape[1],\n",
        "                                 (ymax-ymin)*image.shape[0],\n",
        "                                 linewidth=2,\n",
        "                                 edgecolor='purple',\n",
        "                                 facecolor='none',\n",
        "                                 label='Ground Truth')\n",
        "        ax2.add_patch(rect)\n",
        "\n",
        "    for box, score in zip(pred_boxes, pred_scores):\n",
        "        if score > threshold:\n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            rect = patches.Rectangle((xmin*image.shape[1], ymin*image.shape[0]),\n",
        "                                     (xmax-xmin)*image.shape[1],\n",
        "                                     (ymax-ymin)*image.shape[0],\n",
        "                                     linewidth=2,\n",
        "                                     edgecolor='red',\n",
        "                                     facecolor='none',\n",
        "                                     label='Predicted')\n",
        "            ax2.add_patch(rect)\n",
        "\n",
        "    # Create legend for the annotated image\n",
        "    handles, labels = ax2.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))  # Remove duplicates\n",
        "    ax2.legend(by_label.values(), by_label.keys())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x-6_Z_OUFybx",
        "outputId": "26bbef70-c731-491b-983e-b302576100cb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Sample a random image to inspect\n",
        "idx = np.random.choice(len(test_image_tensors))\n",
        "\n",
        "# Select the first image tensor and its annotations for testing\n",
        "test_image_tensor = validation_tensors[idx][0]\n",
        "test_gt_box_tensor = validation_tensors[idx][1]\n",
        "\n",
        "# Run detection on the single image tensor\n",
        "# @tf.function # Uncomment to run eagerly\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image tensor.\"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    return detections, prediction_dict\n",
        "\n",
        "# Perform detection on the image\n",
        "detections, prediction_dict = detect(test_image_tensor)\n",
        "\n",
        "# Extract detection results\n",
        "detection_boxes = detections['detection_boxes'][0].numpy()\n",
        "detection_classes = detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "detection_scores = detections['detection_scores'][0].numpy()\n",
        "\n",
        "# # Print shapes and contents of the annotations and predictions for understanding\n",
        "# print(\"Ground truth boxes shape:\", test_gt_box_tensor.shape)\n",
        "# print(\"Ground truth boxes content:\", test_gt_box_tensor.numpy())\n",
        "\n",
        "# print(\"Prediction dict keys:\", prediction_dict.keys())\n",
        "# print(\"Detections shape (boxes, classes, scores):\",\n",
        "#       detection_boxes.shape, detection_classes.shape, detection_scores.shape)\n",
        "# print(\"Detections content (boxes):\", detection_boxes)\n",
        "# print(\"Detections content (classes):\", detection_classes)\n",
        "# print(\"Detections content (scores):\", detection_scores)\n",
        "\n",
        "# Validate the model\n",
        "iou_threshold = 0.5\n",
        "mean_iou = validate_model_on_batch(detection_model, validation_tensors, iou_threshold)\n",
        "print(f\"Mean IoU score on validation batch: {mean_iou:.4f}\")\n",
        "\n",
        "# Filter detections based on the score threshold\n",
        "score_threshold = 0.20\n",
        "selected_indices = np.where(detection_scores > score_threshold)[0]\n",
        "selected_boxes = detection_boxes[selected_indices]\n",
        "selected_scores = detection_scores[selected_indices]\n",
        "\n",
        "# Squeeze the batch dimension from the image tensor before plotting\n",
        "squeezed_image = np.squeeze(test_image_tensor.numpy())\n",
        "\n",
        "plot_predictions_and_ground_truth(\n",
        "    image=squeezed_image,\n",
        "    gt_boxes=test_gt_box_tensor.numpy(),\n",
        "    pred_boxes=selected_boxes,\n",
        "    pred_scores=selected_scores,\n",
        "    threshold=score_threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Oljl3R0a72"
      },
      "source": [
        "## Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t64n6KcC0e0m"
      },
      "source": [
        "### A. Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_aEiP2wS9-Ge",
        "outputId": "4ceb00b7-74c4-4b21-f1c0-182f7cc3200b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import unittest\n",
        "\n",
        "# Function to flip an image horizontally using TensorFlow\n",
        "def tf_flip_image_horizontal(image):\n",
        "    return tf.image.flip_left_right(image)\n",
        "\n",
        "# Function to rotate an image by 90 degrees clockwise using TensorFlow\n",
        "def tf_rotate_image_90(image):\n",
        "    return tf.image.rot90(image, k=3)  # k=3 for 90 degrees clockwise rotation\n",
        "\n",
        "# Function to adjust bounding boxes for horizontal flip using TensorFlow\n",
        "def tf_adjust_boxes_for_flip(boxes, image_width):\n",
        "    new_boxes = tf.stack([boxes[:, 0], image_width - boxes[:, 3],\n",
        "                          boxes[:, 2], image_width - boxes[:, 1]], axis=1)\n",
        "    return new_boxes\n",
        "\n",
        "# Function to adjust bounding boxes for 90 degree rotation using TensorFlow\n",
        "def tf_adjust_boxes_for_rotation(boxes, image_height):\n",
        "    new_boxes = tf.stack([boxes[:, 1], image_height - boxes[:, 2],\n",
        "                          boxes[:, 3], image_height - boxes[:, 0]], axis=1)\n",
        "    return new_boxes\n",
        "\n",
        "# TensorFlow unit test class\n",
        "class TFImageAugmentationTest(tf.test.TestCase):\n",
        "    def test_image_augmentation(self):\n",
        "        # Create a mock image and bounding boxes\n",
        "        image = tf.zeros((640, 640, 3), dtype=tf.uint8)\n",
        "        image = image + tf.cast(tf.stack([\n",
        "            tf.pad(tf.ones((100, 100, 3), dtype=tf.uint8) * 255, [[100, 440], [100, 440], [0, 0]]),\n",
        "            tf.pad(tf.ones((100, 100, 3), dtype=tf.uint8) * 255, [[400, 140], [300, 240], [0, 0]])\n",
        "        ]), tf.uint8)\n",
        "        boxes = tf.constant([[100, 100, 200, 200], [400, 300, 500, 400]], dtype=tf.float32)\n",
        "\n",
        "        image_width, image_height = image.shape[1], image.shape[0]\n",
        "\n",
        "        # Apply horizontal flip\n",
        "        flipped_image = tf_flip_image_horizontal(image)\n",
        "        flipped_boxes = tf_adjust_boxes_for_flip(boxes, image_width)\n",
        "\n",
        "        # Apply 90 degree rotation\n",
        "        rotated_image = tf_rotate_image_90(flipped_image)\n",
        "        rotated_boxes = tf_adjust_boxes_for_rotation(flipped_boxes, image_height)\n",
        "\n",
        "        # Check if the boxes still correctly identify the objects\n",
        "        for box in rotated_boxes:\n",
        "            ymin, xmin, ymax, xmax = tf.cast(box, tf.int32).numpy()\n",
        "            object_area = rotated_image[ymin:ymax, xmin:xmax]\n",
        "            # Assert that the object area is all white (255)\n",
        "            self.assertAllEqual(tf.reduce_all(tf.equal(object_area, 255)), True)\n",
        "\n",
        "        print(\"Test passed successfully!\")\n",
        "\n",
        "# Instantiate the test case and run the test method\n",
        "test_case = TFImageAugmentationTest()\n",
        "test_case.test_image_augmentation()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vptz7cb1CShg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
